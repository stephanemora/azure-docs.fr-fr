### YamlMime:FAQ
metadata:
  title: FAQ sur Azure Video Analyzer – Azure
  description: Cette article répond aux questions fréquemment posées sur Azure Video Analyzer.
  ms.service: azure-video-analyzer
  ms.topic: conceptual
  ms.date: 06/01/2021
  ms.openlocfilehash: d7cd8c04c55f27426486ed0853f2450946e5c07b
  ms.sourcegitcommit: f6e2ea5571e35b9ed3a79a22485eba4d20ae36cc
  ms.translationtype: HT
  ms.contentlocale: fr-FR
  ms.lasthandoff: 09/24/2021
  ms.locfileid: "128637484"
title: FAQ sur Azure Video Analyzer
summary: Cette article répond aux questions couramment posées sur Azure Video Analyzer.
sections:
- name: Ignoré
  questions:
  - question: Général
    answer: "**Quelles variables système puis-je utiliser dans la définition de topologie de pipeline ?**\n\n| Variable   |  Description  | \n| --- | --- | \n| System.Runtime.DateTime | Représente un instant en heure UTC, généralement exprimé sous la forme d’une date et d’une heure dans la journée selon le format suivant :<br>*yyyyMMddTHHmmssZ* | \n| System.Runtime.PreciseDateTime | Représente une instance de date-heure en temps universel coordonné (UTC, Universal Time Coordinated) dans un format compatible avec les fichiers ISO8601 avec des millisecondes, au format suivant :<br>*yyyyMMddTHHmmss.fffZ* | \n| System.TopologyName    | Représente le nom de la topologie de pipeline. | \n| System.PipelineName | Représente le nom du pipeline en direct. | \n\n> [!Note] \n> System.Runtime.DateTime and System.Runtime.PreciseDateTime ne peut pas être utilisé dans le nom d’une ressource vidéo Azure Video Analyzer, dans un nœud récepteur vidéo. Ces variables peuvent être utilisées dans un nœud FileSink pour nommer le fichier.\n\n**Quelle est la politique de confidentialité pour Video Indexer ?**\n\nVideo Indexer est couvert par la [Déclaration de confidentialité Microsoft](https://privacy.microsoft.com/privacystatement). La Déclaration de confidentialité décrit les données personnelles que Microsoft traite, comment et dans quel but Microsoft les traite. Pour en savoir plus sur la confidentialité, visitez le site [Microsoft Trust Center](https://www.microsoft.com/trustcenter).\n"
  - question: Configuration et déploiement
    answer: "**Puis-je déployer le module Edge sur un appareil Windows 10 ?**\n\nOui. Pour plus d’informations, consultez [Configurer des conteneurs Linux sur Windows 10](/virtualization/windowscontainers/deploy-containers/linux-containers).\n\n**J’utilise une machine virtuelle comme simulation de périphérique. Puis-je redimensionner la configuration de la machine virtuelle après le déploiement ?**\n\nOui, vous pouvez [augmenter ou diminuer la taille d’une machine virtuelle](../../virtual-machines/resize-vm.md) après y avoir déployé des modules Edge tels qu’Azure Video Analyzer. Vous devrez peut-être redémarrer le module IoT Edge sur votre machine virtuelle. Pour une machine virtuelle Linux, utilisez la commande `sudo systemctl restart iotedge`. Si des pipelines en direct étaient en cours d’exécution, ils doivent reprendre une fois le redémarrage terminé. Vous verrez des interruptions dans la sortie du pipeline (par exemple : événements d’inférence, enregistrement vidéo) pendant le redémarrage.\n\n**Comment puis-je ajouter des exemples de fichiers vidéo au module de simulation de caméra RTSP sur un appareil IoT Edge ?**\n\nPour ajouter un nouveau fichier vidéo à votre dossier d’appareil IoT Edge actuel : \n\n- [Chargez le fichier vidéo](../../storage/blobs/storage-quickstart-blobs-portal.md#upload-a-block-blob) vers un conteneur de blobs nouveau ou existant dans le compte de stockage Azure.\n- Une fois le chargement terminé, accédez à Vue d’ensemble -> Propriétés et récupérez l’URL du fichier chargé au format `https://<storage-account-name>.blob.core.windows.net/<container-name>/<videofile.mkv>`.\n- Connectez-vous à votre terminal de périphérique et exécutez la commande suivante : `sudo curl https://<storage-account-name>.blob.core.windows.net/<container-name>/<videofile.mkv> --output /home/localedgeuser/samples/input/<videofile.mkv>`. Vous pouvez choisir un autre chemin de sortie de votre choix.\n\nVous trouverez des exemples de fichiers multimédias disponibles pour les tests sur cette page relative aux [jeux de données multimédias](https://github.com/Azure/video-analyzer/tree/main/media).\n"
  - question: Capture des paramètres de caméra IP et RTSP
    answer: "**Dois-je utiliser un Kit de développement logiciel (SDK) spécial sur mon appareil pour envoyer un flux vidéo ?**\n\nNon, Video Analyzer prend en charge la capture de média à l’aide du protocole RTSP (Real-Time Streaming Protocol) de diffusion vidéo en continu que prennent en charge la plupart des caméras IP.\n\n**Puis-je envoyer un média à Video Analyzer en utilisant des protocoles autres que RTSP ?**\n\nNon, Video Analyzer ne prend en charge que le protocole RTSP pour la capture de vidéo à partir de caméras IP. Toute caméra qui prend en charge le streaming RTSP sur TCP/HTTP doit être compatible. \n\n**Puis-je réinitialiser ou mettre à jour l’URL source RTSP dans un pipeline en direct ?**\n\nOui, quand le pipeline en direct est à l’état *inactif*.  \n\n**Existe-t-il un simulateur RTSP pouvant être utilisé pendant les tests et le développement ?**\n\nOui, il existe un module de périphérie de [simulateur RTSP](https://github.com/Azure/video-analyzer/tree/main/edge-modules/sources/rtspsim-live555), qui peut être utilisé dans les didacticiels et les guides de démarrage rapide afin de prendre en charge le processus d’apprentissage. Ce module est fourni en guise de service Meilleur effort et peut ne pas toujours être disponible. Nous vous recommandons vivement de ne *pas* utiliser le simulateur pendant plus de quelques heures. Nous vous recommandons d’effectuer des tests avec votre source RTSP actuelle avant de planifier un déploiement de production.\n"
  - question: Conception d’un modèle IA
    answer: "**J’ai plusieurs modèles IA enveloppés dans un conteneur Docker. Comment les utiliser avec Azure Video Analyzer ?** \n\nLes solutions varient selon le protocole de communication utilisé par le serveur d’inférence pour communiquer avec Azure Video Analyzer. Les sections suivantes décrivent le fonctionnement de chaque protocole.\n\n*Utiliser le protocole HTTP* :\n\n* Conteneur unique (module nommé *avaextension*) :  \n\n   Dans votre serveur d’inférence, vous pouvez utiliser un seul port, mais des points de terminaison différents selon les modèles IA. Par exemple, pour un échantillon Python, vous pouvez utiliser différentes `routes` par modèle, comme illustré ici : \n\n   ```\n   @app.route('/score/face_detection', methods=['POST']) \n   … \n   Your code specific to face detection model\n\n   @app.route('/score/vehicle_detection', methods=['POST']) \n   … \n   Your code specific to vehicle detection model \n   … \n   ```\n\n   Ainsi, dans votre déploiement Video Analyzer, quand vous activez des pipelines en direct, définissez l’URL du serveur d’inférence pour chacun d’eux comme suit : \n\n   Premier pipeline en direct : URL du serveur d’inférence =`http://avaextension:44000/score/face_detection`<br/>\n   Deuxième pipeline en direct : URL du serveur d’inférence =`http://avaextension:44000/score/vehicle_detection`  \n   \n    > [!NOTE]\n    > Vous pouvez également exposer vos modèles IA sur des ports différents et les appeler quand vous activez les pipelines en direct.  \n\n* Plusieurs conteneurs : \n\n   Chaque conteneur est déployé avec un nom différent. Dans les démarrages rapides et les tutoriels, nous vous avons montré comment déployer une extension nommée *avaextension*. Vous pouvez maintenant développer deux conteneurs différents, chacun avec la même interface HTTP, ce qui signifie qu’ils ont le même point de terminaison `/score`. Déployez ces deux conteneurs avec des noms différents et veillez à ce que les deux écoutent des *ports distincts*. \n\n   Par exemple, un conteneur nommé `avaextension1` écoute le port `44000`, l’autre conteneur nommé `avaextension2` écoute le port `44001`. \n\n   Dans votre topologie Video Analyzer, vous instanciez deux pipelines en direct avec différentes URL d’inférence, comme illustré ici : \n\n   Premier pipeline en direct : URL du serveur d’inférence =`http://avaextension1:44000/score`    \n   Deuxième pipeline en direct : URL du serveur d’inférence =`http://avaextension2:44001/score`\n   \n*Utiliser le protocole gRPC* : \n\n* Le nœud d’extension gRPC a une propriété `extensionConfiguration`, une chaîne facultative utilisable dans la cadre du contrat gRPC. Si vous avez plusieurs modèles d’IA empaquetés dans un seul serveur d’inférence, vous n’aurez pas besoin d’exposer un nœud par modèle d’IA. Au lieu de cela, pour un pipeline en direct, en tant que fournisseur d’extension, vous pouvez définir comment sélectionner les différents modèles IA à l’aide de la propriété `extensionConfiguration`. Pendant l’exécution, Video Analyzer transmet cette chaîne à votre serveur d’inférence qui peut l’utiliser pour appeler le modèle IA souhaité. \n\n**Je crée un serveur gRPC autour d’un modèle IA et je souhaite pouvoir prendre en charge l’utilisation de plusieurs caméras ou pipelines en direct. Comment faire pour créer mon serveur ?** \n\n Tout d’abord, assurez-vous que votre serveur peut traiter plusieurs demandes à la fois ou travailler dans des threads parallèles. \n\nPar exemple, dans l’[exemple gRPC Azure Video Analyzer](./analyze-live-video-use-your-model-grpc.md) suivant, un nombre par défaut de canaux parallèles est défini : \n\n```\nserver = grpc.server(futures.ThreadPoolExecutor(max_workers=3)) \n```\n\nDans l’instanciation de serveur gRPC ci-dessus, le serveur ne peut ouvrir simultanément que trois canaux par caméra ou cinq par pipeline en direct. N’essayez pas de connecter plus de trois instances au serveur. Si vous ouvrez plus de trois canaux, les demandes resteront en attente jusqu’à ce qu’un canal existant abandonne.  \n\nL’implémentation de serveur gRPC précédente est utilisée dans nos exemples Python. En tant que développeur, vous pouvez implémenter votre propre serveur ou utiliser l’implémentation par défaut précédente pour augmenter le numéro de travail, que vous définissez sur le nombre de caméras à utiliser pour les flux vidéo. \n\nPour configurer et utiliser plusieurs caméras, vous pouvez instancier plusieurs pipelines en direct pointant ou non vers le même serveur d’inférence (par exemple le serveur mentionné dans le paragraphe précédent). \n\n**Je souhaite pouvoir recevoir plusieurs trames avant de prendre une décision d’inférence. Comment puis procéder ?** \n\nLes [exemples actuels par défaut](https://github.com/Azure/video-analyzer/tree/main/edge-modules) fonctionnent en mode *sans état*. Ils ne conservent pas l’état des appels précédents ou l’ID de l’appelant. Cela signifie que plusieurs pipelines en direct peuvent appeler le même serveur d’inférence, mais que le serveur ne peut pas distinguer qui appelle ou l’état de l’appelant. \n\n*Utiliser le protocole HTTP* :\n\nPour conserver l’état, chaque appelant, ou pipeline en direct, appelle le serveur d’inférence avec le paramètre de requête HTTP qui lui est propre. Par exemple, les adresses URL de serveur d’inférence pour chaque pipeline en direct sont présentées ici :  \n\nPremier pipeline en direct : `http://avaextension:44000/score?id=1`<br/>\nDeuxième pipeline en direct : `http://avaextension:44000/score?id=2`\n\n… \n\nCôté serveur, l’`id` permet d’identifier l’appelant. Si `id`=1, le serveur peut conserver l’état séparément pour ce pipeline en direct. Il peut ensuite conserver les trames vidéo reçues dans un tampon. Par exemple, utilisez un tableau ou un dictionnaire avec une clé DateTime, et la valeur est le frame. Vous pouvez ensuite définir le serveur à traiter (inférer) après la réception de *x* nombre de trames. \n\n*Utiliser le protocole gRPC* : \n\nAvec une extension gRPC, chaque session est destinée à un seul flux de caméra. Il n’est donc pas nécessaire de fournir un identificateur. Avec la propriété extensionConfiguration, vous pouvez stocker les trames vidéo dans un tampon et définir le serveur à traiter (inférer) après réception de *x* trames. \n\n**Tous les ProcessMediaStreams d’un conteneur particulier exécutent-ils le même modèle d’IA ?** \n\nNon. Les appels de démarrage ou d’arrêt de l’utilisateur final dans un pipeline en direct constituent une session. Il peut aussi y avoir une déconnexion ou reconnexion de caméra. L’objectif est de conserver une unique session si la caméra diffuse de la vidéo. \n\n* Deux caméras envoyant de la vidéo pour traitement (à deux pipelines en direct distincts) créent deux sessions. \n* Une caméra reliée à un pipeline en direct comportant deux nœuds d’extension gRPC crée deux sessions. \n\nChaque session constitue une connexion bidirectionnelle entre Video Analyzer et le serveur gRPC, qui peut avoir son propre modèle. \n\n> [!NOTE]\n> En cas de déconnexion ou reconnexion d’une caméra qui passe hors ligne pendant une période dépassant les limites de tolérance, Video Analyzer ouvre une session avec le serveur gRPC. Le serveur n’a pas besoin d’effectuer le suivi de l’état d’une session à l’autre. \n\nVideo Analyzer ajoute également la prise en charge de plusieurs extensions gRPC pour une seule caméra dans un pipeline en direct. Vous pouvez utiliser ces extensions gRPC pour effectuer le traitement IA de manière séquentielle, en parallèle, voire les deux. \n\n> [!NOTE]\n> Le fait que plusieurs extensions s’exécutent en parallèle affecte vos ressources matérielles. Gardez cela à l’esprit lorsque vous choisissez le matériel adapté à vos besoins de calcul. \n\n**Quel est le nombre maximal de ProcessMediaStreams simultanés ?** \n\nVideo Analyzer n’applique aucune limite.  \n\n**Comment déterminer si mon serveur d’inférence doit utiliser l’accélérateur de processeur, de GPU ou tout autre accélérateur matériel ?** \n\nVotre décision dépend de la complexité du modèle d’IA développé et de la façon dont vous souhaitez utiliser les accélérateurs de processeur et de matériel. Lorsque vous développez le modèle d’IA, vous pouvez préciser les ressources que le modèle doit utiliser et les actions qu’il doit effectuer. \n\n**Comment faire pour afficher les cadres englobants générés par mon serveur d’inférence ?** \n\nVous pouvez enregistrer les résultats de l’inférence avec le média dans votre ressource vidéo. Vous pouvez utiliser un [widget](player-widget.md) pour lire la vidéo avec une superposition des données d’inférence.\n"
  - question: Compatibilité gRPC
    answer: "**Comment savoir quels sont les champs obligatoires du descripteur de flux multimédia ?** \n\nTout champ auquel vous ne fournissez pas de valeur reçoit une [valeur par défaut, comme spécifié par gRPC](https://developers.google.com/protocol-buffers/docs/proto3#default).  \n\nVideo Analyzer utilise la version *proto3* du langage de la mémoire tampon du protocole. Toutes les données de la mémoire tampon du protocole que les contrats Video Analyzer utilisent sont disponibles dans les [fichiers de mémoire tampon du protocole]().<!--add-valid-link.md)--><!--https://github.com/Azure/azree-video-analyzer/tree/master/contracts/grpc-->. \n\n**Comment être sûr d’utiliser les derniers fichiers de mémoire tampon du protocole ?** \n\nVous pouvez obtenir les derniers fichiers de mémoire tampon de protocole sur le site [fichiers de contrat](https://github.com/Azure/video-analyzer/tree/main/contracts/grpc). Chaque fois que nous les mettons à jour, les fichiers de contrat s’affichent à cet emplacement. Il n’existe aucun projet immédiat de mettre à jour les fichiers du protocole, recherchez donc le nom du package en haut des fichiers pour connaître la version. Elle devrait indiquer : \n\n```\nmicrosoft.azure.media.live_video_analytics.extensibility.grpc.v1\n```\n\nToutes les mises à jour de ces fichiers incrémentent la valeur « v-value » à la fin du nom. \n\n> [!NOTE]\n> Étant donné que Video Analyzer utilise la version proto3 du langage, les champs sont facultatifs, ce qui offre une compatibilité ascendante et descendante. \n\n**Quelles sont les fonctionnalités gRPC utilisables avec Video Analyzer ? Lesquelles sont obligatoires et lesquelles facultatives ?** \n\nVous pouvez utiliser toutes les fonctionnalités gRPC côté serveur, à condition que le contrat Protobuf (Protocol Buffers) soit rempli.\n"
  - question: Analyse et métriques
    answer: "**Puis-je surveiller le pipeline à la périphérie à l’aide d’Azure Event Grid ?**\n\nOui. Vous pouvez consommer des [métriques Prometheus](monitor-log-edge.md#azure-monitor-collection-via-telegraf) et les publier dans votre Event Grid. \n\n**Puis-je utiliser Azure Monitor pour afficher l’intégrité, les métriques et les performances de mes pipelines dans le cloud ou à la périphérie ?**\n\nOui, nous prenons en charge cette approche. Pour en savoir plus, consultez [Guide pratique pour utiliser les métriques Azure Monitor.](../../azure-monitor/essentials/data-platform-metrics.md)\n\n**Existe-t-il des outils qui facilitent la surveillance du module IoT Edge Azure Video Analyzer ?**\n\nVisual Studio Code prend en charge l’extension Azure IoT Tools qui vous permet de surveiller facilement les points de terminaison du module Edge Video Analyzer. Vous pouvez utiliser cet outil pour commencer rapidement à analyser les « événements » du point de terminaison intégré IoT Hub et pour voir les messages d’inférence qui sont acheminés du périphérique vers le cloud. \n\nEn outre, vous pouvez utiliser cette extension pour modifier le jumeau du module Edge Video Analyzer afin de redéfinir les paramètres du pipeline.\n\nPour plus d’informations, consultez l’article [Surveillance et enregistrement](monitor-log-edge.md).\n"
  - question: Facturation et disponibilité
    answer: >
      **Comment Azure Video Analyzer est-il facturé ?**


      Pour plus d’informations sur la facturation, consultez la page [Tarification Video Analyzer](https://azure.microsoft.com/pricing/details/video-analyzer/).
additionalContent: "\n## <a name=\"next-steps\"></a>Étapes suivantes\n\n[Démarrage rapide : Bien démarrer avec Azure Video Analyzer](get-started-detect-motion-emit-events.md)\n"
